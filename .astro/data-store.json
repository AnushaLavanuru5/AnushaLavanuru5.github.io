[["Map",1,2,7,8,65,66],"meta::meta",["Map",3,4,5,6],"astro-version","5.0.9","config-digest","7b3656f8503cd3c9","posts",["Map",9,10,28,29],"customizing-user-information",{"id":9,"data":11,"body":22,"filePath":23,"assetImports":24,"digest":26,"deferredRender":27},{"author":12,"publishDate":13,"title":14,"tags":15,"description":18,"cover":19},"VicBox",["Date","2024-12-20T16:20:35.000Z"],"Customizing User Information",[16,17],"general","docs","A complete guide on how to customize user information in this Astro Simple Portfolio theme.",{"src":20,"alt":21},"__ASTRO_IMAGE_./images/customizing-user-information/cover.webp","Customizing AstroPaper theme color schemes","If you’re looking to personalize your Simple Portfolio with Astro Build, one of the first things you’ll want to do is change the default user information. By editing a few fields, you can make the portfolio entirely your own. This quick guide will show you exactly how to do that.\n\n---\n\n1. ## Locate the Configuration File\nAll the user information you need to update is found in one place:\n\nFile: src/config.ts\nObject Name: ME\nOpen src/config.ts in your preferred code editor, and look for the exported ME object. It should look something like this:\n\n```js\nexport const ME = {\n    name: \"John Doe\",\n    profession: \"Software Engineer | Full Stack Developer\",\n    profileImage: \"pp.png\",\n    profileFacts: [\n        {\n            value: 10,\n            description: \"Years of Experience\"\n        },\n        {\n            value: 5,\n            description: \"Completed Projects\"\n        },\n        {\n            value: 4,\n            description: \"Satisfied Clients\"\n        }\n    ],\n    contactInfo: {\n        email: \"vicbox.dev@vicbox.dev\",\n        linkedin: \"https://www.linkedin.com/in/victor-alvaradohn\",\n        resumeDoc: \"resume.pdf\",\n    },\n    aboutMe: \"I am a software engineer with a passion for web development. I have experience in building web \" + \"applications using modern technologies. I am a self-taught developer who enjoys learning new things and \" + \"sharing knowledge with others.\",\n}\n```\n---\n\n2. ## Update Your Basic Information\nWithin the ME object, you’ll see several properties. Simply replace the default values with your own:\n\n- name: Your full name or preferred display name.\n- profession: Your job title, role, or specialty (e.g., “Web Developer,” “Data Scientist,” etc.).\n- profileImage: The filename of your profile image. Make sure this image is located in the appropriate folder (often public or an images folder).\n- aboutMe: A short bio or summary of your background, interests, and expertise.\n\nFor example:\n\n```ts\nexport const ME = {\n  name: \"Jane Doe\",\n  profession: \"Frontend Developer & UI/UX Designer\",\n  profileImage: \"myProfilePic.png\",\n  //...\n  aboutMe: \"I am a passionate frontend developer with a keen eye for design. I love transforming...\"\n};\n```\n\n3. ## Customize the Profile Facts\nThe profileFacts array is designed to showcase highlights such as years of experience, completed projects, or any other metrics you want to feature. Each fact has two fields:\n\nvalue: The numerical figure (e.g., 10, 5, 4).\ndescription: A short text explaining what that number represents.\nFeel free to add, remove, or edit items in this array to match your background. For example:\n\n```ts\nexport const ME = {\n  //...\n  profileFacts: [\n    {\n      value: 10,\n      description: \"Years of Experience\"\n    },\n    {\n      value: 5,\n      description: \"Completed Projects\"\n    },\n    {\n      value: 4,\n      description: \"Satisfied Clients\"\n    }\n  ]\n};\n```\n\n4. ## Update Your Contact Information\nIf you want to include a contact form or social media links, you can update the contactInfo object. Simply replace the default values with your own:\n\n- email: Your email address.\n- linkedin: Your LinkedIn profile URL.\n- resumeDoc: The filename of your resume or CV.\n\nFor example:\n\n```ts\nexport const ME = {\n  //...\n  contactInfo: {\n    email: \"jane.doe@example.com\",\n    linkedin: \"https://www.linkedin.com/in/jane-doe\",\n    resumeDoc: \"resume.pdf\"\n  }\n};\n```\n\n5. ## Customize the About Me Section\nThe aboutMe property is a free-form text field that allows you to share a brief bio or summary of your background, interests, and expertise. Feel free to update this text to reflect your personality and experiences.\n\n6. ## Save Your Changes\nOnce you’ve made your updates, save the file and test your portfolio to ensure everything looks as expected. If you encounter any issues, don’t hesitate to reach out for help.\n\n---\n\n## Conclusion\nCustomizing your Simple Portfolio with Astro Build is as easy as editing one configuration file. By updating the ME object in src/config.ts, you can quickly tailor the entire site to reflect your personal brand and professional experience.\n\nFeel free to get creative—add new facts, change images, and include links to all your social platforms. With just a few tweaks, you’ll have a portfolio that shows off your work in a polished, professional way.\n\nHappy coding, and enjoy your new personalized portfolio!","src/content/posts/customizing-user-information.mdx",[25],"./images/customizing-user-information/cover.webp","c1bf63391610b886",true,"customizing-theme-color-schemes",{"id":28,"data":30,"body":38,"filePath":39,"assetImports":40,"digest":42,"rendered":43},{"author":12,"publishDate":31,"title":32,"tags":33,"description":18,"cover":35},["Date","2024-12-20T15:20:35.000Z"],"Customizing color schemes of Astro Simple Portfolio",[34,17],"color-schemes",{"src":36,"alt":37},"__ASTRO_IMAGE_./images/customizing-theme-color-schemes/cover.webp","Customizing color schemes","Astro Simple Portfolio provides a straightforward way to adapt to your system’s light and dark mode preferences. By default, when a user’s device is set to dark mode, the site will load in dark mode—otherwise, it displays in light mode.\n\n> **Note:** This guide assumes you have a basic understanding of Tailwind CSS and how it’s configured in Astro.\n\n## Table of Contents\n\n1. [Introduction to System-Preferred Light & Dark Mode](#introduction-to-system-preferred-light--dark-mode)\n2. [Enabling or Disabling Dark Mode Support](#enabling-or-disabling-dark-mode-support)\n3. [Updating Tailwind Config](#updating-tailwind-config)\n4. [Testing Your Changes](#testing-your-changes)\n5. [Additional Tips & Best Practices](#additional-tips--best-practices)\n\n---\n\n## Introduction to System-Preferred Light & Dark Mode\n\nBy default, **Astro Simple Portfolio** detects your system preference using the `prefers-color-scheme` media query. This means:\n\n- If your device is set to **light mode**, the site appears in light mode.\n- If your device is set to **dark mode**, the site appears in dark mode.\n\nNo extra toggle or button is included in this template—**the theme switch happens automatically** based on the user’s system settings.\n\n\n## Updating Tailwind Config\nTailwind uses color variables to define how your site looks in light and dark mode. You can modify these variables in tailwind.config.mjs. Below is an example snippet:\n\n```js\n// file: tailwind.config.mjs\ncolors: {\n// Light mode colors\n'light-theme': '#E9EBEC',\n'primary-light': '#FBD144',\n'primary-hover-light': '#FFE071',\n\n// Dark mode colors\n'dark-theme': '#0C151D',\n'primary-dark': '#FFE071',\n'primary-hover-dark': '#FBD144',\n\n// Neutrals\n'n900': '#222222',\n'n700': '#171F26',\n'n500': '#555555',\n}\n```\n### Changing the Default Palette\n1. Choose which colors to modify\n   - Want a new background color? Update light-theme or dark-theme.\n   - Want to tweak highlights or accents? Change primary-light, primary-hover-light, primary-dark, and primary-hover-dark.\n   \n2. Replace the color values\n   For instance, if your brand color is teal, you might set primary-light to #3AB7BF and primary-hover-light to #37A1A8.\n\n3. Save and rebuild\nTailwind will regenerate the necessary utility classes when you save and restart your Astro development server.\n\n---\n\n## Testing Your Changes\n1. Local Development Preview\nRun your local dev server with npm run dev or yarn dev.\n   - Switch your system settings between light and dark mode to ensure the site updates automatically.\n\n2. Cross-Browser Checks\nTest your portfolio in multiple browsers (Chrome, Firefox, Safari, Edge) and devices (desktop, mobile) to confirm everything looks consistent.\n\n3. Accessibility & Contrast\n   - Use tools like WebAIM Contrast Checker to verify your text has high enough contrast against the background.\n   - Ensure buttons, links, and icons remain visible and legible in both modes.","src/content/posts/customizing-theme-color-schemes.md",[41],"./images/customizing-theme-color-schemes/cover.webp","ad74c24a31dcdda4",{"html":44,"metadata":45},"\u003Cp>Astro Simple Portfolio provides a straightforward way to adapt to your system’s light and dark mode preferences. By default, when a user’s device is set to dark mode, the site will load in dark mode—otherwise, it displays in light mode.\u003C/p>\n\u003Cblockquote>\n\u003Cp>\u003Cstrong>Note:\u003C/strong> This guide assumes you have a basic understanding of Tailwind CSS and how it’s configured in Astro.\u003C/p>\n\u003C/blockquote>\n\u003Ch2 id=\"table-of-contents\">Table of Contents\u003C/h2>\n\u003Col>\n\u003Cli>\u003Ca href=\"#introduction-to-system-preferred-light--dark-mode\">Introduction to System-Preferred Light &#x26; Dark Mode\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"#enabling-or-disabling-dark-mode-support\">Enabling or Disabling Dark Mode Support\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"#updating-tailwind-config\">Updating Tailwind Config\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"#testing-your-changes\">Testing Your Changes\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"#additional-tips--best-practices\">Additional Tips &#x26; Best Practices\u003C/a>\u003C/li>\n\u003C/ol>\n\u003Chr>\n\u003Ch2 id=\"introduction-to-system-preferred-light--dark-mode\">Introduction to System-Preferred Light &#x26; Dark Mode\u003C/h2>\n\u003Cp>By default, \u003Cstrong>Astro Simple Portfolio\u003C/strong> detects your system preference using the \u003Ccode>prefers-color-scheme\u003C/code> media query. This means:\u003C/p>\n\u003Cul>\n\u003Cli>If your device is set to \u003Cstrong>light mode\u003C/strong>, the site appears in light mode.\u003C/li>\n\u003Cli>If your device is set to \u003Cstrong>dark mode\u003C/strong>, the site appears in dark mode.\u003C/li>\n\u003C/ul>\n\u003Cp>No extra toggle or button is included in this template—\u003Cstrong>the theme switch happens automatically\u003C/strong> based on the user’s system settings.\u003C/p>\n\u003Ch2 id=\"updating-tailwind-config\">Updating Tailwind Config\u003C/h2>\n\u003Cp>Tailwind uses color variables to define how your site looks in light and dark mode. You can modify these variables in tailwind.config.mjs. Below is an example snippet:\u003C/p>\n\u003Cpre class=\"astro-code plastic\" style=\"background-color:#21252B;color:#A9B2C3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"js\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#5F6672;font-style:italic\">// file: tailwind.config.mjs\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#A9B2C3\">colors: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#5F6672;font-style:italic\">// Light mode colors\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">light-theme\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">: \u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">#E9EBEC\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">primary-light\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">: \u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">#FBD144\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">primary-hover-light\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">: \u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">#FFE071\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#5F6672;font-style:italic\">// Dark mode colors\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">dark-theme\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">: \u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">#0C151D\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">primary-dark\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">: \u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">#FFE071\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">primary-hover-dark\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">: \u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">#FBD144\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#5F6672;font-style:italic\">// Neutrals\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">n900\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">: \u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">#222222\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">n700\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">: \u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">#171F26\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">n500\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">: \u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#98C379\">#555555\u003C/span>\u003Cspan style=\"color:#A9B2C3\">'\u003C/span>\u003Cspan style=\"color:#A9B2C3\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#A9B2C3\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"changing-the-default-palette\">Changing the Default Palette\u003C/h3>\n\u003Col>\n\u003Cli>\n\u003Cp>Choose which colors to modify\u003C/p>\n\u003Cul>\n\u003Cli>Want a new background color? Update light-theme or dark-theme.\u003C/li>\n\u003Cli>Want to tweak highlights or accents? Change primary-light, primary-hover-light, primary-dark, and primary-hover-dark.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>Replace the color values\nFor instance, if your brand color is teal, you might set primary-light to #3AB7BF and primary-hover-light to #37A1A8.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Save and rebuild\nTailwind will regenerate the necessary utility classes when you save and restart your Astro development server.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Chr>\n\u003Ch2 id=\"testing-your-changes\">Testing Your Changes\u003C/h2>\n\u003Col>\n\u003Cli>\n\u003Cp>Local Development Preview\nRun your local dev server with npm run dev or yarn dev.\u003C/p>\n\u003Cul>\n\u003Cli>Switch your system settings between light and dark mode to ensure the site updates automatically.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>Cross-Browser Checks\nTest your portfolio in multiple browsers (Chrome, Firefox, Safari, Edge) and devices (desktop, mobile) to confirm everything looks consistent.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Accessibility &#x26; Contrast\u003C/p>\n\u003Cul>\n\u003Cli>Use tools like WebAIM Contrast Checker to verify your text has high enough contrast against the background.\u003C/li>\n\u003Cli>Ensure buttons, links, and icons remain visible and legible in both modes.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ol>",{"headings":46,"imagePaths":64,"frontmatter":30},[47,51,54,57,61],{"depth":48,"slug":49,"text":50},2,"table-of-contents","Table of Contents",{"depth":48,"slug":52,"text":53},"introduction-to-system-preferred-light--dark-mode","Introduction to System-Preferred Light & Dark Mode",{"depth":48,"slug":55,"text":56},"updating-tailwind-config","Updating Tailwind Config",{"depth":58,"slug":59,"text":60},3,"changing-the-default-palette","Changing the Default Palette",{"depth":48,"slug":62,"text":63},"testing-your-changes","Testing Your Changes",[],"projects",["Map",67,68,88,89,108,109,126,127,143,144,161,162,182,183],"hgr",{"id":67,"data":69,"body":84,"filePath":85,"assetImports":86,"digest":87,"deferredRender":27},{"title":70,"startDate":71,"endDate":72,"summary":73,"url":74,"cover":75,"tags":76,"ogImage":83},"Hand Gesture Recognition System",["Date","2023-11-01T00:00:00.000Z"],["Date","2023-12-01T00:00:00.000Z"],"A real-time machine learning-based hand gesture recognition system developed using computer vision and deep learning techniques to enhance human-computer interaction for applications in automation, gaming, accessibility, and defense.","https://github.com/yourusername/hand-gesture-recognition","__ASTRO_IMAGE_./images/hgr/hgrhandgesture.jpg",[77,78,79,80,81,82],"Python","OpenCV","TensorFlow","Keras","Computer Vision","HCI","./images/hgr/hgrhandgesture.jpg","## Table of Contents\n\n1. [Overview](#overview)\n2. [Tech Stack](#tech-stack)\n3. [Key Features](#key-features)\n4. [Project Highlights](#project-highlights)\n5. [Outcome](#outcome)\n6. [Links](#links)\n\n---\n\n## Overview\n\nThe Hand Gesture Recognition System is a machine learning-based solution designed to interpret static hand gestures in real time. The system enhances **human-computer interaction (HCI)** across domains such as smart automation, immersive gaming, and assistive technologies.\n\nThe primary goal was to design a model capable of identifying hand gestures under varying lighting, background complexity, and positioning, with high accuracy. The system translates physical hand signs into digital commands, offering an intuitive, touchless way to control devices and software.\n\n---\n\n## Tech Stack\n\n- **Programming Languages:** Python  \n- **Libraries and Tools:** OpenCV, TensorFlow, Keras  \n- **Dataset:** NUS Hand Posture Dataset\n\nThe project was developed using **Python**, taking advantage of **OpenCV** for image processing and camera integration. Deep learning models were built with **TensorFlow** and **Keras** to classify gestures from segmented image inputs. The training and testing were done using the **NUS Hand Posture Dataset**, which provides a comprehensive set of labeled hand gestures captured in diverse environments. This dataset ensured that the model could generalize well to real-world variability.\n\n---\n\n## Key Features\n\nThe system leverages robust feature extraction techniques, including **Histogram of Oriented Gradients (HOG)** and **Fibonacci Weighted Neighborhood Patterns (FWNP)**. These helped capture the critical structure of hand shapes and ensured consistent recognition even with subtle posture differences.\n\nA dedicated real-time pipeline was implemented using a standard webcam to detect gestures on the fly. Preprocessing steps like background segmentation, noise filtering, and edge detection were optimized to isolate the hand region with precision.\n\nPerformance was enhanced by introducing adaptability in the model, allowing it to function effectively across dynamic environments  including changes in lighting, scale, and gesture angle.\n\n![Architecture](./images/hgr/hgrarch.png)\n\n---\n\n## Outcome\n\nThe model delivered **high gesture classification accuracy**, maintaining consistent performance under varied conditions. User interaction was found to be fluid and responsive, greatly improving the experience of HCI through hand recognition.\n\nThis project demonstrated how a combination of classical image processing and modern neural networks can result in scalable, practical systems with applications in automation, assistive tech, and human-computer collaboration.\n\n![Outcome](./images/hgr/hgrhandgesture.jpg)\n\n---\n\n## Project Highlights\n\nThis system unlocks multiple real-world use cases. For instance, it supports **touchless control** of smart home devices, enabling users to operate appliances without physical contact. In **gaming and entertainment**, it can provide immersive control inputs, expanding player interaction without external controllers.\n\nFrom an accessibility standpoint, the system is capable of assisting in **sign language interpretation**, bridging communication gaps for the hearing-impaired. Additionally, the model’s robustness makes it suitable for **defense applications**, where secure and gesture-based interaction systems are increasingly valuable.\n\n---\n\n## Links\n\n- [GitHub Repository](https://github.com/yourusername/hand-gesture-recognition)  \n- [Research Documentation](https://yourdomain.com/hgr-docs)\n\n---","src/content/projects/hgr.mdx",[83],"33844ea90f427763","emo-verse",{"id":88,"data":90,"body":104,"filePath":105,"assetImports":106,"digest":107,"deferredRender":27},{"title":91,"startDate":92,"endDate":93,"summary":94,"url":95,"cover":96,"tags":97,"ogImage":103},"Emoverse – AR Platform for Emotional Expression",["Date","2023-12-01T00:00:00.000Z"],["Date","2023-12-31T00:00:00.000Z"],"Emoverse is an Augmented Reality platform designed to foster emotional well-being among university students by allowing them to externalize and share emotions in a virtual space, promoting empathy and asynchronous peer support.","https://yourdomain.com/emoverse","__ASTRO_IMAGE_./images/emoverse/emoversebanner.png",[98,99,100,101,102],"Augmented Reality","Figma","UX","Emotional Wellbeing","Mental Health","./images/emoverse/emoversebanner.png","## Table of Contents\n\n1. [Overview](#overview)\n2. [Tech Stack](#tech-stack)\n3. [Key Features](#key-features)\n4. [Project Highlights](#project-highlights)\n5. [Applications](#applications)\n6. [Impact](#impact)\n7. [Future Scope](#future-scope)\n8. [Links](#links)\n\n---\n\n## Overview\n\n**Emoverse** is an Augmented Reality (AR) platform built to promote emotional well-being, especially among university students navigating academic stress and personal challenges. The platform encourages users to **externalize their emotions** through AR objects, which are shared in a collaborative virtual environment.\n\nThese objects, paired with descriptive text, form the basis for asynchronous and empathetic interaction. Other users can respond to emotional expressions by interacting with AR elements  such as poking, patting, or replying  fostering **community-driven support** without the pressure of real-time conversation.\n\n---\n\n## Tech Stack\n\n- **Platform:** Augmented Reality (AR) using tools like Luma  \n- **Prototype Design:** Figma  \n- **Backend & Testing:** Email-based Wizard-of-Oz setup for simulation\n\nThe system was prototyped using **Figma** to design the interface and user journey, while AR experiences were conceptualized using **Luma**. Early-stage interactions were tested via a **Wizard-of-Oz setup**, where backend logic was simulated through email-based user responses to validate core design hypotheses before full implementation.\n\n---\n\n## Key Features\n\nEmoverse allows users to **create symbolic AR objects** representing emotional states. These are not just static visuals  they come paired with **personal reflections**, enabling contextual understanding and richer emotional communication.\n\nUsers engage in a **virtual emotional space**, where they can interact with shared AR content by touching, poking, or patting  translating physical gestures into virtual empathy. Over time, these interactions evolve into **support threads**, capturing a journey of shared emotional experiences.\n\nThe system is intentionally asynchronous, which provides users with the flexibility to engage **on their own terms**, promoting deeper, more thoughtful interactions that aren’t limited by time or presence.\n\n![System Structure](./images/emoverse/emoversestruct.png)\n\n---\n\n## Project Highlights\n\nA key success of the project was its **user-centered design**, validated through user studies that showed measurable improvements in mood and emotional state. Participants engaged with the platform and reported reductions in anxiety and stress after emotional expression.\n\nThe prototype was iteratively refined in **Figma**, incorporating features like **customizable AR visuals**, **descriptive text prompts**, and a fluid interface for interaction. These elements were tested and validated for usability and emotional resonance.\n\nEmoverse also demonstrated that **emotion sharing through AR**, even without real-time presence, can still create meaningful community support and psychological relief.\n\n![Prototype Design](./images/emoverse/prototypedesign.png)\n\n---\n\n## Applications\n\nThe Emoverse concept can be extended across several domains. In **mental health support**, it facilitates **non-intrusive emotional reflection** and **peer validation**  which are critical for student well-being.\n\nIn **therapeutic settings**, it provides a lightweight, expressive tool that can supplement counseling and mindfulness practices. Additionally, it supports **community building** in both academic and broader social contexts, offering a platform for empathy-driven interactions in digital space.\n\n---\n\n## Impact\n\nUser studies revealed strong **quantitative outcomes**, including a **50% reduction in negative moods** and a **12% drop in anxiety** within 24 hours of using Emoverse. These outcomes demonstrate the tangible impact of emotional externalization through interactive AR.\n\nUsers described the experience as “**heartwarming**,” highlighting the importance of pairing emotional visuals with written context. Feedback underscored how even asynchronous interactions created a sense of **human connection** and comfort.\n\n![User Study Results](./images/emoverse/quantitativeresults.png)\n\n---\n\n## Future Scope\n\nFuture development will explore **real-time interaction capabilities**, such as direct chat and feedback for more immediate emotional support. There’s also potential for **customizable AR emotion objects**, allowing users greater expressive control.\n\nIn addition, Emoverse could be expanded through **larger-scale user studies** in therapeutic, academic, and social settings to evaluate its use in long-term emotional well-being and mental health strategies.\n\n---\n\n## Links\n\n- [View Emoverse Prototype on Figma](https://yourdomain.com/emoverse-figma)  \n- [Research Paper on Emoverse](https://yourdomain.com/emoverse-paper)\n\n---","src/content/projects/emo-verse.mdx",[103],"a96d118768338e8f","ar-harlem",{"id":108,"data":110,"body":122,"filePath":123,"assetImports":124,"digest":125,"deferredRender":27},{"title":111,"startDate":112,"endDate":113,"summary":114,"url":115,"cover":116,"tags":117,"ogImage":121},"Community History Through AR",["Date","2024-02-01T00:00:00.000Z"],["Date","2024-03-02T00:00:00.000Z"],"An Augmented Reality application developed to preserve Harlem’s cultural and historical identity, allowing residents to explore and contribute to community narratives through interactive AR experiences.","https://yourdomain.com/ar-harlem","__ASTRO_IMAGE_./images/arharlem/harlembanner.png",[98,118,119,120,100,99],"Unity","Blender","Cultural Preservation","./images/arharlem/harlembanner.png","## Table of Contents\n\n1. [Overview](#overview)\n2. [Tech Stack](#tech-stack)\n3. [Responsibilities](#responsibilities)\n4. [Key Features](#key-features)\n5. [Future Scope](#future-scope)\n6. [Applications](#applications)\n7. [Links](#links)\n\n---\n\n## Overview\n\n**Community History Through AR** is an Augmented Reality application developed to preserve and celebrate the cultural and historical identity of Harlem, New York. As the neighborhood faces ongoing gentrification, this platform empowers local residents to reconnect with their roots by engaging with AR-enhanced narratives tied to landmarks, buildings, and shared community stories.\n\nInspired by features from previous projects like *Emoverse*, the app allows users to leave behind digital memories  AR assets, comments, and stories  at real-world locations. These contributions create a living, community-driven archive that fosters emotional connection and cultural continuity.\n\n---\n\n## Tech Stack\n\n- **Programming Languages:** Python  \n- **Libraries:** Unity, Blender, Figma  \n- **Tools:** Adobe Illustrator, Balsamiq\n\nThe development process involved **Unity** and **Blender** for prototyping 3D models and interactive AR scenes, while **Figma** and **Balsamiq** were used for designing wireframes and refining user interaction flows. Supporting visuals were developed in **Adobe Illustrator**, ensuring a consistent and accessible UI. The use of Python scripts further supported backend logic and data handling in testing phases.\n\n---\n\n## Responsibilities\n\nThe project began with **community research**, including field interviews, archival exploration, and collaboration with key stakeholders such as the **NSF Center for Smart Streetscapes (CS3)** and the **125th Street Business Improvement District**. This ensured that the application design aligned with community needs and historical integrity.\n\nIn terms of **UI/UX design**, I created multi-layered wireframes in Figma, incorporating interactive elements like **heatmaps** (inspired by Snapchat) to highlight Harlem’s key landmarks and storytelling locations. These were themed around local history, safety, health, and art.\n\nOn the technical side, I developed AR features using Unity. This included 3D modeling of historic buildings in Blender to enable time-based visualization, allowing users to view how a site looked in the past versus today. I also integrated features that allowed users to place and view **location-bound AR content**, further enriched by community input.\n\nTo validate our concept, a **pilot study** was conducted with 10 participants using a beta version of the app. Feedback from this study was used to iterate on core design and functionality.\n\n![Main Image](./images/arharlem/arharlem.png)\n\n---\n\n## Key Features\n\nOne of the central features is the **heatmap functionality**, which visualizes historical engagement across Harlem  drawing attention to important cultural and architectural points.\n\nThe app also includes **AR storytelling capabilities**, where users can point their phones at buildings or landmarks and see overlays of historical photos, events, and community-submitted narratives.\n\nTo further immerse users, **interactive maps** were built that highlight stories under key community themes, including public safety, health, and local art initiatives. These maps adapt in real time as users contribute new content or explore different parts of the neighborhood.\n\n![Results](./images/arharlem/arharlemresults.png)\n\n---\n\n## Future Scope\n\nAs the project evolves, several directions for future development have been identified:\n\n- **Enhanced User Experience:** Refining navigation and UI interactions to make the platform more intuitive and accessible across devices.\n- **User Studies:** Conducting expanded studies with Harlem residents to validate and scale the platform, ensuring it remains grounded in authentic community needs.\n- **Research Publication:** Documenting findings and methodologies to publish a paper on the intersection of AR and cultural preservation, contributing to research in community-based design and immersive technology.\n\n---\n\n## Applications\n\nThis project has broad interdisciplinary applications:\n\n- In **cultural preservation**, it provides communities with tools to reclaim and protect their narratives.\n- As an **educational resource**, it introduces immersive ways for students and institutions to engage with local history.\n- For **civic engagement**, it encourages shared storytelling that can strengthen neighborhood identity and social cohesion.\n\n---\n\n## Links\n\n- [Explore Community History Through AR Features](https://yourdomain.com/ar-harlem-app)  \n- [Research Paper on AR and Cultural Preservation](https://yourdomain.com/ar-harlem-research)\n\n---","src/content/projects/ar-harlem.mdx",[121],"959ad4097455cb51","imagegan",{"id":126,"data":128,"body":139,"filePath":140,"assetImports":141,"digest":142,"deferredRender":27},{"title":129,"startDate":130,"endDate":131,"summary":132,"url":133,"cover":134,"tags":135,"ogImage":138},"Image Enhancement using GANs",["Date","2024-01-01T00:00:00.000Z"],["Date","2024-01-31T00:00:00.000Z"],"A deep learning project that uses Generative Adversarial Networks (GANs) to enhance the visual quality of images. It focuses on improving image sharpness, contrast, and clarity using a custom U-Net GAN architecture and advanced training techniques.","https://github.com/yourusername/image-enhancement-gans","__ASTRO_IMAGE_./images/imggan/imageganbanner.png",[77,79,136,80,81,137],"GAN","Image Processing","./images/imggan/imageganbanner.png","## Table of Contents\n\n1. [Overview](#overview)\n2. [Tech Stack](#tech-stack)\n3. [Key Features](#key-features)\n4. [Project Highlights](#project-highlights)\n5. [Applications](#applications)\n6. [Outcome](#outcome)\n7. [Links](#links)\n\n---\n\n## Overview\n\nThis project focuses on **image enhancement using Generative Adversarial Networks (GANs)** to improve the visual quality of low-resolution or noisy images. The GAN architecture was tailored to boost image sharpness, brightness, and contrast  making it suitable for applications where detail clarity is critical.\n\nChallenges related to recovering information from degraded images were addressed by integrating both local and global enhancement mechanisms into the model. The architecture was evaluated on a wide variety of inputs and proved effective in producing visually superior outputs, even under harsh imaging conditions.\n\n---\n\n## Tech Stack\n\n- **Programming Languages:** Python  \n- **Libraries:** OpenCV, TensorFlow, Keras  \n- **Dataset:** MIT-Adobe 5K (5,000 high-resolution images)\n\nThe model was built using **Python** and implemented with **TensorFlow** and **Keras** for deep learning workflows. **OpenCV** was used for image preprocessing and visualization. Training and evaluation were performed on the **MIT-Adobe 5K dataset**, which includes diverse high-resolution photos ideal for enhancement tasks. The dataset’s variability allowed testing under real-world conditions, ensuring robustness across lighting and noise levels.\n\n---\n\n## Key Features\n\nThe model leverages a **custom U-Net-based generator**, capable of capturing both fine-grained details and high-level image structure. Global image features are injected into the decoder stages to balance local sharpness with contextual consistency.\n\nTo improve training dynamics, a **Wasserstein GAN (WGAN)** variant was employed. This helped achieve faster convergence and better gradient stability. An **adaptive weighting mechanism** was introduced to prioritize difficult samples and dynamically adjust learning focus.\n\nThe enhancement process was further refined by **automating critical preprocessing steps** like edge detection, noise reduction, and region-based segmentation  allowing the model to receive clearer, more focused inputs.\n\n![GAN Architecture](./images/imggan/ganarch.png)\n\n---\n\n## Project Highlights\n\nOne of the standout accomplishments was the **significant improvement in Peak Signal-to-Noise Ratio (PSNR)** over traditional enhancement baselines. This metric reflects the model’s ability to restore image fidelity and reduce distortion.\n\nThe training strategy also incorporated **cycle consistency** using a dual-direction GAN setup. This ensured that enhancements did not introduce visual artifacts and could be reversed with minimal degradation. Additionally, **SSIM (Structural Similarity Index)** was used to evaluate perceptual quality and validate improvements across varying image conditions  including low light, motion blur, and poor exposure.\n\n---\n\n## Applications\n\nThis system has real-world applicability across several domains. It can be used for **photo enhancement** in photography platforms and social media pipelines to increase visual appeal with minimal user effort. In **medical imaging**, it can enhance structural visibility in scans, supporting better diagnosis and analysis.\n\nIt is also applicable in **underwater photography**, where lighting conditions are often poor and color fidelity is distorted. The architecture is extendable to **real-time video processing**, opening up possibilities in security footage enhancement and media post-production.\n\n---\n\n## Outcome\n\nThe project resulted in a **scalable and efficient image enhancement pipeline**, capable of running near real-time on modern hardware. Its modular design and generalizability make it well-suited for adaptation across a wide range of use cases  from everyday photo editing to high-stakes medical and environmental imaging.\n\nIn terms of performance, the system consistently delivered cleaner, sharper, and more accurate visual outputs, verified both through quantitative metrics and human evaluation.\n\n---\n\n## Links\n\n- [GitHub Repository](https://github.com/yourusername/image-enhancement-gans)  \n- [Research Documentation](https://yourdomain.com/your-doc-link)\n\n---","src/content/projects/imagegan.mdx",[138],"86c7d5ba45e1f8a4","pol-xr",{"id":143,"data":145,"body":157,"filePath":158,"assetImports":159,"digest":160,"deferredRender":27},{"title":146,"startDate":147,"endDate":148,"summary":149,"url":150,"cover":151,"tags":152,"ogImage":156},"PolXR – Immersive XR for Polarimetric Data",["Date","2024-05-01T00:00:00.000Z"],["Date","2024-05-01T00:00:00.000Z"],"PolXR is an immersive XR platform developed under Columbia University's VISER initiative that enables researchers to explore polarimetric geospatial data in 3D. Built using Unity, MRTK, and XR libraries, it revolutionizes scientific exploration in environmental and climate studies.","https://pgg.ldeo.columbia.edu/projects/VISER/pol-XR","__ASTRO_IMAGE_./images/polxr/polxrbanner.jpg",[118,153,154,119,155],"XR","MRTK","C#","./images/polxr/polxrbanner.jpg","## Table of Contents\n\n1. [Overview](#overview)\n2. [Tech Stack](#tech-stack)\n3. [Key Features](#key-features)\n4. [Project Highlights](#project-highlights)\n5. [Applications](#applications)\n6. [Future Scope](#future-scope)\n7. [Related Links](#related-links)\n\n---\n\n## Video Demo\n\n\u003Ciframe width=\"100%\" height=\"400\" src=\"https://www.youtube-nocookie.com/embed/9MUUbKgprP8?si=WGQXX1OxJ-aPIaKk\"\ntitle=\"YouTube video player\" frameborder=\"0\"\nallow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\nreferrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen>\u003C/iframe>\n\n---\n\n## Overview\n\n**PolXR** is an extended reality platform designed to visualize and explore polarimetric geospatial datasets in immersive 3D environments. Developed under Columbia University’s **VISER (Visualization for Enhanced Scientific Exploration in Research)** initiative, the project is intended to support climate and environmental research through more intuitive, interactive data analysis.\n\nBy leveraging cutting-edge XR capabilities, PolXR empowers researchers to interact with data in virtual or augmented reality. This allows for deeper pattern recognition, spatial reasoning, and a better grasp of complex environmental datasets. The system is tailored not only for scientific accuracy but also for usability  bridging the gap between raw data and human insight.\n\n---\n\n## Tech Stack\n\n- **Programming Languages:** C#\n- **Libraries:** XR, MRTK (Mixed Reality Toolkit)\n- **Tools:** Unity, Blender\n\nPolXR was developed in **Unity** using **C#**, chosen for its real-time rendering power and compatibility with XR devices. The interface and interaction design rely heavily on **MRTK (Mixed Reality Toolkit)**, which streamlines cross-platform development for both AR and VR. Blender was used for creating and importing 3D assets, ensuring high-fidelity visuals while keeping performance optimized for immersive environments.\n\n---\n\n## Key Features\n\nAt the heart of PolXR is its ability to **transform raw polarimetric data into a spatial experience**. The platform enables immersive exploration by rendering geospatial structures as 3D objects, letting researchers zoom, rotate, and manipulate the view to uncover hidden relationships in the data.\n\nPolXR also supports **cross-platform compatibility**, functioning across both VR and AR hardware, which makes it versatile for field and lab use. The **interactive interface** includes intuitive controls for data filtering, scaling, and rotation  built specifically with scientific workflows in mind. Perhaps most importantly, PolXR integrates smoothly with existing polarimetric workflows and geospatial formats, making it easy to adopt without reworking data pipelines.\n\n![PolXR Login](./images/polxr/loginpolxr.png)\n\n---\n\n## Project Highlights\n\nPolXR was developed as part of the **Polar Geospatial Research Group** at Columbia University. The project reflects a commitment to building tools that help scientists and policymakers gain insight through innovation in spatial visualization.\n\nA key focus of the development process was **user-centered design**  the interface and interaction model were refined through iterative testing with domain researchers to ensure clarity, accessibility, and speed of use. The result is a tool that doesn’t just visualize data but facilitates **discovery**.\n\n![PolXR Screen](./images/polxr/screen2polxr.png)\n\n---\n\n## Applications\n\nPolXR’s immersive environment has a wide range of potential use cases. In **environmental research**, it offers new ways to track and analyze polarimetric signals tied to climate change. For **education**, it can serve as a powerful demonstration tool, helping students understand geospatial patterns that are difficult to grasp in 2D.\n\nIn **policy and collaboration**, PolXR supports clear communication of complex geospatial phenomena. Decision-makers can step into the data, gain perspective, and collaborate more effectively with scientists through shared XR experiences.\n\n---\n\n## Future Scope\n\nThere are clear directions for expanding PolXR’s functionality. One major area is **multi-user collaboration**, allowing teams of researchers to explore and annotate the same 3D space in real time, regardless of location. This could revolutionize remote fieldwork and scientific presentations.\n\nAnother area involves the integration of **real-time data streams**, which would allow users to visualize environmental change as it happens. In addition, advanced analytics like **predictive modeling** could be layered into the environment to simulate outcomes and analyze trends directly within the immersive space.\n\n---\n\n## Related Links\n\n- [🔗 Explore the Polar Geophysics Group](https://pgg.ldeo.columbia.edu/projects/VISER/pol-XR)  \n- [🔗 PolXR GitHub Repository](https://github.com/ColumbiaCGUI/PolXR/tree/networking-fall2024)\n\n---","src/content/projects/pol-xr.mdx",[156],"006f72d32c1b1435","pos-tag",{"id":161,"data":163,"body":178,"filePath":179,"assetImports":180,"digest":181,"deferredRender":27},{"title":164,"startDate":165,"endDate":166,"summary":167,"url":168,"cover":169,"tags":170,"ogImage":177},"NLP POS Tagging & Autocorrection",["Date","2024-06-01T00:00:00.000Z"],["Date","2024-06-01T00:00:00.000Z"],"A hands-on NLP project exploring statistical and neural approaches to POS tagging and spell correction. Implemented in Python with PyTorch and evaluated across multiple languages.","https://github.com/AnushaLavanuru5/POSTagging_AutoCorrection","__ASTRO_IMAGE_./images/postag/postagbanner.jpeg",[77,171,172,173,174,175,176],"NLP","PyTorch","HMM","RNN","LSTM","Jupyter","./images/postag/postagbanner.jpeg","## Table of Contents\n\n1. [Overview](#overview)\n2. [Tech Stack](#tech-stack)\n3. [Model Design](#model-design)\n4. [Project Highlights](#project-highlights)\n5. [Evaluation & Impact](#evaluation--impact)\n6. [Applications](#applications)\n7. [Future Scope](#future-scope)\n8. [Links](#links)\n\n---\n\n## Overview\n\n**NLP POS Tagging and Autocorrection** is a two-part project that investigates fundamental and advanced techniques in natural language processing. The work revolves around two core tasks: **Part-of-Speech (POS) tagging** and **spell correction**, both of which are crucial components in most NLP pipelines.\n\nThe goal of the project was to build models from scratch using both traditional probabilistic methods and modern deep learning architectures. Along the way, I focused heavily on performance comparisons, error analysis, and multilingual generalization, which allowed for a deep understanding of the capabilities and limitations of each approach.\n\n---\n\n## Tech Stack\n\n- **Programming Languages:** Python  \n- **Libraries:** NumPy, PyTorch  \n- **Tools:** Matplotlib, Jupyter Notebooks\n\nThe entire project was developed using **Python** due to its strong support for scientific computing and machine learning. **NumPy** was used for core array operations and data transformations, while **PyTorch** powered the training of deep learning models. Visual analysis and debugging were done with **Matplotlib**, and the entire workflow  from preprocessing to evaluation  was managed in **Jupyter Notebooks**, making experiments easy to document, iterate, and reproduce.\n\n---\n\n## Model Design\n\nFor the POS tagging task, I began with **Hidden Markov Models (HMMs)** using both bigram and trigram sequences. These probabilistic models provided a strong baseline and helped in understanding language structure from a statistical perspective.\n\nTo go further, I implemented **Recurrent Neural Networks (RNNs)**, **Long Short-Term Memory (LSTM)** networks, and **Bidirectional LSTMs**. These models were trained on labeled corpora and evaluated on both accuracy and sentence-level correctness. The Bidirectional LSTM, unsurprisingly, performed best due to its ability to leverage both past and future context  essential in tagging ambiguous or complex sentence structures.\n\nIn parallel, the autocorrection component was based on **n-gram language models** (unigram, bigram, trigram), which were enhanced using **Laplace smoothing** and **backoff strategies**. These were used to detect and correct spelling mistakes by predicting the most probable word given its context.\n\n![Main Screenshot](./images/postag/postagmain.png)\n\n---\n\n## Project Highlights\n\nA big focus of this project was **comparative evaluation**. I ran all models across several datasets and analyzed performance across different language types  including English, Japanese, and Bulgarian  to understand how morphology and sentence structure influence tagging accuracy and error rates.\n\nI also visualized learning curves to diagnose underfitting, overfitting, and convergence speed for each model. The insights gained here helped optimize training regimes and model complexity. Performance results and plots were compiled to clearly present how traditional methods stack up against neural architectures  not just in accuracy, but also in training efficiency and robustness.\n\n---\n\n## Evaluation & Impact\n\nThe models were evaluated on **token-level accuracy**, **sentence-level correctness**, and **correction accuracy** (for the autocorrect task). What stood out was that neural models  particularly BiLSTMs  consistently outperformed HMMs in POS tagging, especially when dealing with longer sequences or noisy input.\n\nFrom the autocorrection side, trigram models with smoothing performed significantly better than simpler methods, particularly in real-world typos and context-heavy sentences.\n\nBeyond metrics, the project gave me a sharper understanding of how NLP models generalize across languages, and what kinds of patterns are hardest to learn  such as rare words, non-standard grammar, or homophones. The visualizations and result summaries were especially well-received in peer review and presentations.\n\n![Results](./images/postag/postagresults.png)\n\n---\n\n## Applications\n\nThe techniques developed here have direct relevance in NLP systems like **grammar correction tools**, **chatbots**, and **language learning platforms**. Accurate POS tagging enhances syntactic parsing, which feeds into downstream tasks like named entity recognition and machine translation. Meanwhile, reliable spell correction is essential for user-facing applications that deal with raw text input, such as messaging platforms or browser extensions.\n\nThe multilingual focus also adds value for global deployment of NLP tools, where different languages present very different linguistic challenges.\n\n---\n\n## Future Scope\n\nThere’s a lot of room to build on this foundation. The next steps would be to:\n\n- Extend POS tagging and autocorrection to more languages and dialects, especially those with low-resource datasets.\n- Develop real-time spell correction capabilities that could integrate into editors or browsers.\n- Implement **transformer-based models** like **BERT** or **RoBERTa**, which have shown state-of-the-art performance in many NLP benchmarks. These could offer significant improvements, particularly in edge cases where sequential models like LSTMs fall short.\n\n---\n\n## Links\n\n[🔗 View Repository on GitHub](https://github.com/AnushaLavanuru5/POSTagging_AutoCorrection)","src/content/projects/pos-tag.mdx",[177],"46af117fae5fa7c0","probing-gpt-2",{"id":182,"data":184,"body":195,"filePath":196,"assetImports":197,"digest":198,"deferredRender":27},{"title":185,"startDate":186,"endDate":187,"summary":188,"url":189,"cover":190,"tags":191,"ogImage":194},"Probing GPT-2 Layers for Understanding and Relationship Analysis",["Date","2024-08-01T00:00:00.000Z"],["Date","2024-08-30T00:00:00.000Z"],"A deep dive into the hidden layers of GPT-2 using structural probing techniques to analyze how linguistic relationships like entailment, contradiction, and neutrality are understood by different layers of the model.","https://github.com/AnushaLavanuru5/ProbingGPT2","__ASTRO_IMAGE_./images/gpt2probe/gpt2banner.png",[77,192,193,171,172],"GPT-2","Transformers","./images/gpt2probe/gpt2banner.png","## Table of Contents\n\n1. [Overview](#overview)\n2. [Tech Stack](#tech-stack)\n3. [Key Features](#key-features)\n4. [Project Highlights](#project-highlights)\n5. [Impact](#impact)\n6. [Applications](#applications)\n7. [Future Scope](#future-scope)\n8. [Related Links](#related-links)\n\n---\n\n## Overview\n\nThe **Probing GPT-2 Layers for Understanding and Relationship Analysis** project explores the internal workings of GPT-2 using structural probing. The aim is to uncover how different layers of the transformer model understand linguistic relationships such as **entailment**, **contradiction**, and **neutrality**.\n\nProbing in this context involves extracting hidden states from specific layers and analyzing their performance on relationship classification tasks. By examining how information is structured and interpreted across GPT-2's architecture, the project provides insights into the distribution of semantic understanding across the model's depth.\n\n---\n\n## Tech Stack\n\n- **Programming Languages:** Python  \n- **Libraries:** Transformers, Baukit, Scikit-learn  \n- **Tools:** PyTorch, Google Colab\n\nThe project was developed in **Python**, utilizing the **Transformers** library for accessing GPT-2 internals. **Baukit** was used for model dissection and hidden state access, while **Scikit-learn** provided the necessary tools for probing and classification. The entire workflow was executed in **Google Colab**, taking advantage of its GPU environment and ease of visualization. **PyTorch** was the core framework powering both model interaction and training tasks.\n\n---\n\n## Key Features\n\nThis project presents a detailed **layer-by-layer probing analysis** of GPT-2. Hidden states from multiple points including shallow like `h.0.mlp`, intermediate like `h.3.mlp`, and deeper layers like `h.9.attn` were examined to assess how the model processes and builds semantic understanding.\n\nOne of the key focal points was **semantic relationship classification**. The system was tasked with identifying entailment, contradiction, and neutrality between sentence pairs, allowing a performance comparison across different GPT-2 layers.\n\nStructural probing provided insights into **which layers contribute most** to tasks requiring nuanced understanding. The project explored both feed-forward and attention blocks to understand **layer interaction dynamics** and how contextual data flows through the model.\n\nPerformance was measured using quantitative metrics, enabling comparisons and highlighting which layers specialize in tasks such as **paraphrase detection**, **sentiment analysis**, or **logical inference**. Additionally, the findings demonstrated how some layers adapt more effectively to specific types of input, offering practical guidance for model interpretability in downstream applications.\n\n![Main Visualization](./images/gpt2probe/gpt2main.png)\n\n---\n\n## Project Highlights\n\nA core highlight of this work was the layer-specific evaluation pipeline designed to operate seamlessly within Google Colab. The project successfully mapped the **semantic capabilities of different GPT-2 layers**, showing the increasing depth of understanding from shallow to deep representations.\n\nDeeper layers, particularly those involving attention heads, were found to contribute most significantly to capturing linguistic relationships. The project’s clean architecture also made it easy to plug in additional datasets and models for comparative analysis.\n\n![Results](./images/gpt2probe/gptresults.png)\n\n---\n\n## Impact\n\nThrough structural probing, the project **quantified the semantic gains** across layers, demonstrating that deeper layers improved performance in relationship classification tasks.\n\nThe work was recognized for its contribution to **model transparency**, making it easier to understand what GPT-2 “knows” and where that knowledge is concentrated. By identifying specific layers responsible for semantic interpretation, the project opens the door for better fine-tuning and transfer learning strategies.\n\n---\n\n## Applications\n\nThis project has multiple practical and academic applications. It supports **linguistic analysis** by shedding light on how transformer models internally encode meaning. It also acts as an **educational tool** for teaching the principles of probing, interpretability, and model internals.\n\nFor developers and researchers working with LLMs, this approach can be used for **model debugging**, helping pinpoint layer-level performance issues or inefficiencies during fine-tuning.\n\n---\n\n## Future Scope\n\nThere are several directions in which this work can expand. One of the next steps involves **multi-model probing**, extending the same evaluation to models like BERT, RoBERTa, and T5 to explore architectural differences in semantic encoding.\n\nAdditionally, future development may involve **predictive modeling**, **interactive visualizations**, and **real-time probing tools** to analyze layer behaviors during inference on live inputs.\n\n---\n\n## Related Links\n\n- [SNLI Dataset](https://nlp.stanford.edu/projects/snli/)  \n- [GenAI GitHub Repository](https://github.com/AnushaLavanuru5/ProbingGPT2)\n\n---","src/content/projects/probing-gpt-2.mdx",[194],"e63a86c5797f4d1a"]